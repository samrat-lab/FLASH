{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b323af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir as ls\n",
    "from os import makedirs as mkd\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scipy.io.arff import loadarff\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37d280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp=['Methylome', 'RNA1', 'RNA2','RNA_mix']\n",
    "inp_src='../Input/prep/'\n",
    "inp_raw='../Input/raw/'\n",
    "out_src='../Output/Script1/Results/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4029259",
   "metadata": {},
   "source": [
    "# Always run above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6675a",
   "metadata": {},
   "source": [
    "# 1. Boostrapped t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29268c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading config file for each dataset\n",
    "my_files=glob('config_*.json')# Replace the pattern to match file name as per your convention\n",
    "print('Total number of datasets ',len(my_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ddbacff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runing following code will create a bs_test_[dataset] for each dataset, which has p-value for each samples subset of features\n",
    "for file in tqdm(my_files):\n",
    "    !python get_bs_ttest.py --inp_file {file}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552faeb",
   "metadata": {},
   "source": [
    "# 2. Scoring using the bootstrap matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accd8a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files=glob(inp_src+'*/bs_test*.csv')# Replace the pattern to match file name as per your convention\n",
    "len(my_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431f5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the new significance score\n",
    "def get_score(x):\n",
    "    \n",
    "    '''\n",
    "    get_score function calulcate filter score by taking the average of significant p-values.\n",
    "    '''\n",
    "    sig_score = (x>2).sum()/1000\n",
    "    \n",
    "    \n",
    "    return sig_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e51b0",
   "metadata": {},
   "source": [
    "## 2.1 Calculating score from boot strap matrix filelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0eb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate a list with filter score calculated for each dataset\n",
    "my_scores = []\n",
    "for itm in tqdm(my_files):\n",
    "    score_df = pd.read_csv(itm)\n",
    "    gene_scores = score_df.apply(lambda x: get_score(x), axis=0)\n",
    "    my_scores.append(gene_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e451d",
   "metadata": {},
   "source": [
    "## 2.2 Saving the score from list obtained in matched name file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72fbf563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use my_files to iterate over all the dataset and with corresponding score from my_scores to obtain multiple cutoff points\n",
    "\n",
    "cnt = 0\n",
    "for my_file in my_files:\n",
    "    \n",
    "    base_loc = my_file.split('bs_test')[0]# result_location\n",
    "    file_key = my_file.split('bs_test_')[-1].split('.')[0]#extracting file name\n",
    "    \n",
    "    #creating complete score file\n",
    "    score_file = my_scores[cnt]#fetching score\n",
    "    score_size = len(score_file)\n",
    "    score_all_df = pd.DataFrame({\"Genes\":score_file.index,\"Score\":score_file.values}).sort_values('Score',ascending =False)\n",
    "    sorted_score = score_file.sort_values(ascending = False)\n",
    "    \n",
    "    #Storing the result\n",
    "    score_all_df.to_csv(base_loc+f'score_bs_complete_{file_key}.csv',index=False)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8947f8",
   "metadata": {},
   "source": [
    "# 3. Performing recurive feature elimination on the filtered subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dafbdc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of filtered feature files,on which RFE has to be performed\n",
    "pattern = 'method2_score_bs_filter_toprnk_4percent_[!GSE]'# Replace the pattern to match file name as per your convention\n",
    "my_files=glob(f'{inp_src}*/{pattern}*.csv')\n",
    "len(my_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2138f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing RFE on the final features\n",
    "for feat_file in tqdm(my_files):\n",
    "    f_tag = feat_file.split('percent_')[-1].split('.')[0]\n",
    "    res_name = feat_file.split('\\\\')[-1].split('.')[0]\n",
    "    config_file = f'./configs/config_{f_tag}.json'\n",
    "   \n",
    "    !python perform_rfe.py --model svm --inp_file {config_file} --gene_file {feat_file} --res_file {res_name} --step_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271d6a4",
   "metadata": {},
   "source": [
    "# 4. Comparison of models for final features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e67e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of final features ready for comparision with different models\n",
    "pattern = 'RFE_svm_step1_score_bs_filter_toprnk_4percent'# Replace the pattern to match file name as per your convention\n",
    "my_files=glob(f'{inp_src}*/{pattern}*.csv')\n",
    "len(my_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abcb0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing comparison on final feature for multiple ML models\n",
    "for feat_file in tqdm(my_files):\n",
    "    f_tag = feat_file.split('_toprnk_')[-1].split('.')[0]\n",
    "    config_tag = feat_file.split('percent_')[-1].split('.')[0]\n",
    "    config_file = f'./configs/config_{config_tag}.json'\n",
    "    res_name = f'{pattern}_{f_tag}'\n",
    "    #print(config_file,feat_file,res_name)\n",
    "    !python compare_ml_models.py --inp_file {config_file} --gene_file {feat_file} --res_file {res_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6be2a",
   "metadata": {},
   "source": [
    "# ----------------------------------------------End----------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
